import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import pandas as pd
import re
import os
from kaggle_secrets import UserSecretsClient

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
user_secrets = UserSecretsClient()
hf_token = user_secrets.get_secret("HF_TOKEN") if user_secrets else None

if not torch.cuda.is_available():
    raise RuntimeError("‚ö†Ô∏è GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω! –í–∫–ª—é—á–∏ 'Accelerator T4 x2' –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö.")

model_id = "Qwen/Qwen3-14B" 
input_file = "/kaggle/input/maindata/LR1.csv" 
submission_file = "/kaggle/working/submission.csv"
reasoning_file = "/kaggle/working/reasoning_log.csv"

# === –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ===
print(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ {model_id}...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    token=hf_token,
    trust_remote_code=True
)

# === –§–£–ù–ö–¶–ò–Ø –û–ë–†–ê–ë–û–¢–ö–ò ===
def chat_with_model(category, question, options_str):
    options_list = options_str.split(";;;")
    num_options = len(options_list)

    system_role = f"""<system>
  <role>–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –≤ –æ–±–ª–∞—Å—Ç–∏ {category}.</role>

  <task>
    <goal>–í—ã–±–µ—Ä–∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –æ—Ç–≤–µ—Ç–∞.</goal>
    <method>–°–Ω–∞—á–∞–ª–∞ —Å–¥–µ–ª–∞–π –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –ø—Ä–æ–π–¥–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –≤–∞—Ä–∏–∞–Ω—Ç—É –∏ –æ–±—ä—è—Å–Ω–∏, –ø–æ—á–µ–º—É –æ–Ω –ø–æ–¥—Ö–æ–¥–∏—Ç –∏–ª–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç.</method>
  </task>

  <constraints>
    <numbering>–í–∞—Ä–∏–∞–Ω—Ç—ã –Ω—É–º–µ—Ä—É—é—Ç—Å—è —Å 0.</numbering>
    <answer_format>–í —Å–∞–º–æ–º –∫–æ–Ω—Ü–µ –≤—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –∏–Ω–¥–µ–∫—Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –≤ –¥–≤–æ–π–Ω—ã—Ö –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö: [[N]]. –ù–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ.</answer_format>
    <options_note>–ï—Å–ª–∏ –≤–∞—Ä–∏–∞–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø—è—Ç—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, "—è–±–ª–æ–∫–æ, –±–∞–Ω–∞–Ω"), —ç—Ç–æ –û–î–ò–ù –≤–∞—Ä–∏–∞–Ω—Ç –æ—Ç–≤–µ—Ç–∞, –∞ –Ω–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ.</options_note>
  </constraints>

  <example>
    <question>–ö–∞–∫–∞—è –ø–ª–∞–Ω–µ—Ç–∞ –±–ª–∏–∂–µ –≤—Å–µ–≥–æ –∫ –°–æ–ª–Ω—Ü—É?</question>
    <options>
      <option index="0">–í–µ–Ω–µ—Ä–∞</option>
      <option index="1">–ú–∞—Ä—Å</option>
      <option index="2">–ó–µ–º–ª—è</option>
      <option index="3">–ú–µ—Ä–∫—É—Ä–∏–π</option>
    </options>
    <reasoning>–í–µ–Ω–µ—Ä–∞ ‚Äî –≤—Ç–æ—Ä–∞—è –ø–ª–∞–Ω–µ—Ç–∞ –æ—Ç –°–æ–ª–Ω—Ü–∞, –ú–∞—Ä—Å ‚Äî —á–µ—Ç–≤–µ—Ä—Ç–∞—è, –ó–µ–º–ª—è ‚Äî —Ç—Ä–µ—Ç—å—è. –ú–µ—Ä–∫—É—Ä–∏–π –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –±–ª–∏–∂–µ –≤—Å–µ—Ö –∫ –°–æ–ª–Ω—Ü—É.</reasoning>
    <final_answer>[[3]]</final_answer>
  </example>
</system>"""

    formatted_options = "\n".join([f"{i}. {opt}" for i, opt in enumerate(options_list)])
    prompt = f"–í–æ–ø—Ä–æ—Å: {question}\n\n–í–∞—Ä–∏–∞–Ω—Ç—ã:\n{formatted_options}\n\n–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –æ—Ç–≤–µ—Ç:"

    messages = [
        {"role": "system", "content": system_role},
        {"role": "user", "content": prompt}
    ]

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to("cuda")

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=2500,
            do_sample=True,
            temperature=0.1,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞
    match = re.search(r'\[\[(\d+)\]\]', response)
    ans_index = int(match.group(1)) if match else None
    
    if ans_index is None:
        match = re.search(r'(?:–û—Ç–≤–µ—Ç|Answer)[:\s\-]+(\d+)', response[-500:], re.I)
        ans_index = int(match.group(1)) if match else None
    
    torch.cuda.empty_cache()
    return ans_index, response

# === –û–ë–†–ê–ë–û–¢–ö–ê ===
df = pd.read_csv(input_file)
df['id'] = df.get('Unnamed: 0', df.index)
df['options'] = df['options'].apply(lambda x: ";;;".join(re.findall(r"'([^']*)'", x)))

# –°–æ–∑–¥–∞—ë–º –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã
pd.DataFrame(columns=['id', 'answer']).to_csv(submission_file, index=False)
pd.DataFrame(columns=['question_id', 'model_reasoning']).to_csv(reasoning_file, index=False)

print(f"üèÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(df)} –≤–æ–ø—Ä–æ—Å–æ–≤...\n")

for _, row in df.iterrows():
    try:
        ans, reasoning = chat_with_model(row['category'], row['question'], row['options'])
        
        print(f"–í–æ–ø—Ä–æ—Å {row['id']}: {ans if ans is not None else 0}")
        
        pd.DataFrame([{'id': row['id'], 'answer': ans or 0}]).to_csv(
            submission_file, mode='a', header=False, index=False
        )
        pd.DataFrame([{'question_id': row['id'], 'model_reasoning': reasoning.replace('\n', ' ')}]).to_csv(
            reasoning_file, mode='a', header=False, index=False
        )
        
    except Exception as e:
        print(f"–í–æ–ø—Ä–æ—Å {row['id']}: –û–®–ò–ë–ö–ê - {e}")

print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ!\nüìÑ {submission_file}\nüìÑ {reasoning_file}")