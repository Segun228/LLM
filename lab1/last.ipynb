{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0933a1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.57.3\n",
      "Uninstalling transformers-4.57.3:\n",
      "  Successfully uninstalled transformers-4.57.3\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.5-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m785.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (0.49.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-macosx_14_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface_hub in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (0.36.0)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-1.3.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (3.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from huggingface_hub) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from requests->transformers) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading transformers-4.57.5-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.49.1-py3-none-macosx_14_0_arm64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, transformers\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.49.0\n",
      "    Uninstalling bitsandbytes-0.49.0:\n",
      "      Successfully uninstalled bitsandbytes-0.49.0\n",
      "Successfully installed bitsandbytes-0.49.1 transformers-4.57.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (0.49.1)\n",
      "Requirement already satisfied: transformers in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (4.57.5)\n",
      "Requirement already satisfied: accelerate in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: tqdm in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from bitsandbytes) (2.9.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (3.20.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: psutil in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from requests->transformers) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/karlkorhonen/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install -U transformers accelerate bitsandbytes huggingface_hub\n",
    "!pip install pandas numpy bitsandbytes transformers accelerate scikit-learn tqdm\n",
    "!pip install -U bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb46fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Literal, Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d856f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_tokenizer(\n",
    "    model_name=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    trust_remote_code=True\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Устройство: {device}\")\n",
    "    print(f\"Модель: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=trust_remote_code\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    try:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        print(\"Загружено в 4-bit режиме!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"4-bit не сработало: {e}\")\n",
    "        \n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=trust_remote_code,\n",
    "            )\n",
    "            print(\"Загружено в FP16 режиме!\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"FP16 не сработало: {e2}\")\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=trust_remote_code,\n",
    "            )\n",
    "            print(\"Загружено в базовом режиме!\")\n",
    "    \n",
    "    model.eval()\n",
    "    model.config.use_cache = True\n",
    "    \n",
    "    try:\n",
    "        if hasattr(model, \"get_memory_footprint\"):\n",
    "            memory_gb = model.get_memory_footprint() / 1e9\n",
    "            print(f\"Память модели: {memory_gb:.2f} GB\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_tokenizer(\"Qwen/Qwen2.5-14B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"physics\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of physics</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. State your decisions with the laws of physics</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"chemistry\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of chemistry</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Base your analysis on chemical principles and reactions</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"biology\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of biology</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Use biological principles and evidence</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"economics\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of economics</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Apply economic theories and models</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"math\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of mathematics</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Use mathematical proofs and logical deduction</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"health\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of health and medicine</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Base your analysis on medical knowledge and evidence</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"psychology\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of psychology</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Use psychological theories and research findings</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"history\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of history</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Base your analysis on historical facts and evidence</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"law\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of law</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Apply legal principles and precedents</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"computer science\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of computer science</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Use computational thinking and CS principles</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"engineering\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of engineering</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Apply engineering principles and practical knowledge</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"business\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of business</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Use business concepts and market knowledge</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"philosophy\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert in the field of philosophy</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Apply philosophical reasoning and critical thinking</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\",\n",
    "\n",
    "    \"other\": \"\"\"\n",
    "<system>\n",
    "    <role>You are an experienced expert with broad general knowledge</role>\n",
    "\n",
    "    <task>\n",
    "        <data>You will be given a question and a list of possible answers</data>\n",
    "        <goal>You need to choose only one correct option</goal>\n",
    "        <method>First provide step-by-step reasoning and argumentation: go through each option and explain why it is suitable or not suitable. Use logical analysis and factual knowledge</method>\n",
    "    </task>\n",
    "\n",
    "    <constraints>\n",
    "        <indexation>Options are indexed starting from 0.</indexation>\n",
    "    </constraints>\n",
    "\n",
    "    <answer>\n",
    "        <reasoning>Give a full explanation of your chain of thoughts</reasoning>\n",
    "        <format>At the end give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "    </answer>\n",
    "\n",
    "    <emotional>\n",
    "        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "    </emotional>\n",
    "</system>\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "FEW_SHOT_PROMPTS = {\n",
    "    \"physics\": \"\"\"\n",
    "<question>What is the relationship between force, mass, and acceleration according to Newton's second law?</question>\n",
    "<options>\n",
    "<option index=\"0\">F = m/a</option>\n",
    "<option index=\"1\">F = m × a</option>\n",
    "<option index=\"2\">F = a/m</option>\n",
    "<option index=\"3\">F = m + a</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall Newton's second law of motion.\n",
    "Step 2: Option 0: F = m/a - This is incorrect; it would mean force decreases with increasing acceleration.\n",
    "Step 3: Option 1: F = m × a - This matches the standard formulation: Force equals mass times acceleration.\n",
    "Step 4: Option 2: F = a/m - This is the inverse relationship and is incorrect.\n",
    "Step 5: Option 3: F = m + a - This is dimensionally inconsistent and physically meaningless.\n",
    "Step 6: Option 1 is scientifically accurate and matches the fundamental law.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 1</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"chemistry\": \"\"\"\n",
    "<question>Which element has the atomic number 6?</question>\n",
    "<options>\n",
    "<option index=\"0\">Oxygen</option>\n",
    "<option index=\"1\">Nitrogen</option>\n",
    "<option index=\"2\">Carbon</option>\n",
    "<option index=\"3\">Boron</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall the periodic table and atomic numbers.\n",
    "Step 2: Option 0: Oxygen - Atomic number 8, not 6.\n",
    "Step 3: Option 1: Nitrogen - Atomic number 7, not 6.\n",
    "Step 4: Option 2: Carbon - Atomic number is exactly 6. This is correct.\n",
    "Step 5: Option 3: Boron - Atomic number 5, not 6.\n",
    "Step 6: Carbon is the element with atomic number 6.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 2</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"biology\": \"\"\"\n",
    "<question>Which organelle is responsible for cellular respiration?</question>\n",
    "<options>\n",
    "<option index=\"0\">Nucleus</option>\n",
    "<option index=\"1\">Chloroplast</option>\n",
    "<option index=\"2\">Mitochondrion</option>\n",
    "<option index=\"3\">Ribosome</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Identify the process of cellular respiration.\n",
    "Step 2: Option 0: Nucleus - Controls cell activities but doesn't perform respiration.\n",
    "Step 3: Option 1: Chloroplast - Performs photosynthesis, not respiration.\n",
    "Step 4: Option 2: Mitochondrion - Known as the \"powerhouse of the cell\" for ATP production via respiration.\n",
    "Step 5: Option 3: Ribosome - Protein synthesis, not respiration.\n",
    "Step 6: Mitochondrion is the correct organelle for cellular respiration.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 2</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"economics\": \"\"\"\n",
    "<question>What does GDP stand for in economics?</question>\n",
    "<options>\n",
    "<option index=\"0\">Gross Domestic Product</option>\n",
    "<option index=\"1\">General Domestic Profit</option>\n",
    "<option index=\"2\">Government Debt Percentage</option>\n",
    "<option index=\"3\">Global Development Parameter</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall standard economic terminology.\n",
    "Step 2: Option 0: Gross Domestic Product - Standard definition of GDP.\n",
    "Step 3: Option 1: General Domestic Profit - Not a standard economic term.\n",
    "Step 4: Option 2: Government Debt Percentage - This would be debt/GDP ratio, not GDP itself.\n",
    "Step 5: Option 3: Global Development Parameter - Not the correct expansion of GDP.\n",
    "Step 6: Gross Domestic Product is the universally accepted definition.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 0</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"math\": \"\"\"\n",
    "<question>What is the derivative of x² with respect to x?</question>\n",
    "<options>\n",
    "<option index=\"0\">x</option>\n",
    "<option index=\"1\">2x</option>\n",
    "<option index=\"2\">x³/3</option>\n",
    "<option index=\"3\">2</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Apply the power rule of differentiation: d/dx(xⁿ) = n·xⁿ⁻¹.\n",
    "Step 2: Option 0: x - This would be derivative of x²/2, not x².\n",
    "Step 3: Option 1: 2x - Correct: n=2, so derivative = 2·x²⁻¹ = 2x.\n",
    "Step 4: Option 2: x³/3 - This is the integral, not derivative.\n",
    "Step 5: Option 3: 2 - This would be derivative of 2x, not x².\n",
    "Step 6: The derivative of x² is 2x.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 1</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"health\": \"\"\"\n",
    "<question>Which vitamin is produced by the human body when exposed to sunlight?</question>\n",
    "<options>\n",
    "<option index=\"0\">Vitamin A</option>\n",
    "<option index=\"1\">Vitamin C</option>\n",
    "<option index=\"2\">Vitamin D</option>\n",
    "<option index=\"3\">Vitamin K</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall vitamins and their sources.\n",
    "Step 2: Option 0: Vitamin A - Obtained from food, not sunlight synthesis.\n",
    "Step 3: Option 1: Vitamin C - From fruits/vegetables, not sunlight.\n",
    "Step 4: Option 2: Vitamin D - Skin synthesizes it from sunlight exposure (UVB rays).\n",
    "Step 5: Option 3: Vitamin K - Produced by gut bacteria and from food.\n",
    "Step 6: Vitamin D is the correct answer.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 2</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"history\": \"\"\"\n",
    "<question>In which year did World War II end?</question>\n",
    "<options>\n",
    "<option index=\"0\">1943</option>\n",
    "<option index=\"1\">1944</option>\n",
    "<option index=\"2\">1945</option>\n",
    "<option index=\"3\">1946</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall historical facts about WWII.\n",
    "Step 2: Option 0: 1943 - War was still ongoing (Allies advancing).\n",
    "Step 3: Option 1: 1944 - D-Day happened, but war continued.\n",
    "Step 4: Option 2: 1945 - Germany surrendered May 1945, Japan September 1945.\n",
    "Step 5: Option 3: 1946 - War had already ended.\n",
    "Step 6: 1945 is the universally accepted end year.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 2</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"law\": \"\"\"\n",
    "<question>What is the highest court in the United States federal judiciary?</question>\n",
    "<options>\n",
    "<option index=\"0\">District Court</option>\n",
    "<option index=\"1\">Court of Appeals</option>\n",
    "<option index=\"2\">Supreme Court</option>\n",
    "<option index=\"3\">Federal Circuit Court</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall the structure of US federal courts.\n",
    "Step 2: Option 0: District Court - Trial courts, not highest.\n",
    "Step 3: Option 1: Court of Appeals - Intermediate appellate courts.\n",
    "Step 4: Option 2: Supreme Court - Highest court in the federal system.\n",
    "Step 5: Option 3: Federal Circuit Court - Specialized appeals court.\n",
    "Step 6: The Supreme Court is the highest federal court.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 2</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"computer science\": \"\"\"\n",
    "<question>Which data structure uses LIFO (Last In, First Out) principle?</question>\n",
    "<options>\n",
    "<option index=\"0\">Queue</option>\n",
    "<option index=\"1\">Stack</option>\n",
    "<option index=\"2\">Linked List</option>\n",
    "<option index=\"3\">Tree</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall data structure properties.\n",
    "Step 2: Option 0: Queue - Uses FIFO (First In, First Out).\n",
    "Step 3: Option 1: Stack - Uses LIFO (Last In, First Out).\n",
    "Step 4: Option 2: Linked List - Can implement various access patterns.\n",
    "Step 5: Option 3: Tree - Hierarchical structure, not specifically LIFO.\n",
    "Step 6: Stack is the data structure using LIFO.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 1</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"engineering\": \"\"\"\n",
    "<question>What does CAD stand for in engineering?</question>\n",
    "<options>\n",
    "<option index=\"0\">Computer-Aided Design</option>\n",
    "<option index=\"1\">Computer-Assisted Drawing</option>\n",
    "<option index=\"2\">Calculated Architectural Design</option>\n",
    "<option index=\"3\">Creative Automation Development</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall standard engineering terminology.\n",
    "Step 2: Option 0: Computer-Aided Design - Standard industry term.\n",
    "Step 3: Option 1: Computer-Assisted Drawing - Similar but not the official acronym.\n",
    "Step 4: Option 2: Calculated Architectural Design - Not the standard meaning.\n",
    "Step 5: Option 3: Creative Automation Development - Incorrect expansion.\n",
    "Step 6: Computer-Aided Design is the correct and standard meaning.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 0</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"psychology\": \"\"\"\n",
    "<question>Who is considered the founder of psychoanalysis?</question>\n",
    "<options>\n",
    "<option index=\"0\">Carl Jung</option>\n",
    "<option index=\"1\">Sigmund Freud</option>\n",
    "<option index=\"2\">B.F. Skinner</option>\n",
    "<option index=\"3\">Ivan Pavlov</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall history of psychology.\n",
    "Step 2: Option 0: Carl Jung - Analytical psychology, not founder of psychoanalysis.\n",
    "Step 3: Option 1: Sigmund Freud - Widely recognized as founder of psychoanalysis.\n",
    "Step 4: Option 2: B.F. Skinner - Behaviorism, not psychoanalysis.\n",
    "Step 5: Option 3: Ivan Pavlov - Classical conditioning, not psychoanalysis.\n",
    "Step 6: Sigmund Freud is the correct answer.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 1</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"business\": \"\"\"\n",
    "<question>What does ROI stand for in business?</question>\n",
    "<options>\n",
    "<option index=\"0\">Return on Investment</option>\n",
    "<option index=\"1\">Rate of Interest</option>\n",
    "<option index=\"2\">Revenue Operating Index</option>\n",
    "<option index=\"3\">Risk of Investment</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall standard business metrics.\n",
    "Step 2: Option 0: Return on Investment - Standard business term for profitability measure.\n",
    "Step 3: Option 1: Rate of Interest - Different concept (interest rates).\n",
    "Step 4: Option 2: Revenue Operating Index - Not a standard business acronym.\n",
    "Step 5: Option 3: Risk of Investment - Related but not what ROI stands for.\n",
    "Step 6: Return on Investment is the correct expansion.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 0</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"philosophy\": \"\"\"\n",
    "<question>Who wrote \"The Republic\", discussing justice and the ideal state?</question>\n",
    "<options>\n",
    "<option index=\"0\">Aristotle</option>\n",
    "<option index=\"1\">Plato</option>\n",
    "<option index=\"2\">Socrates</option>\n",
    "<option index=\"3\">Confucius</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall philosophical works and authors.\n",
    "Step 2: Option 0: Aristotle - Wrote \"Politics\", not \"The Republic\".\n",
    "Step 3: Option 1: Plato - \"The Republic\" is Plato's most famous work.\n",
    "Step 4: Option 2: Socrates - Didn't write texts; Plato recorded his teachings.\n",
    "Step 5: Option 3: Confucius - Chinese philosophy, not \"The Republic\".\n",
    "Step 6: Plato is the author of \"The Republic\".\n",
    "</reasoning>\n",
    "<answer>ANSWER: 1</answer>\n",
    "\"\"\",\n",
    "\n",
    "    \"other\": \"\"\"\n",
    "<question>What is the capital city of Australia?</question>\n",
    "<options>\n",
    "<option index=\"0\">Sydney</option>\n",
    "<option index=\"1\">Melbourne</option>\n",
    "<option index=\"2\">Canberra</option>\n",
    "<option index=\"3\">Brisbane</option>\n",
    "</options>\n",
    "<reasoning>\n",
    "Step 1: Recall geography and capitals.\n",
    "Step 2: Option 0: Sydney - Largest city but not capital.\n",
    "Step 3: Option 1: Melbourne - Cultural center, was temporary capital, not current.\n",
    "Step 4: Option 2: Canberra - Specifically built as capital; correct answer.\n",
    "Step 5: Option 3: Brisbane - Major city but not capital.\n",
    "Step 6: Canberra is the capital of Australia.\n",
    "</reasoning>\n",
    "<answer>ANSWER: 2</answer>\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc351a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROMPTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mLLM\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen3-14B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mllm_few_shot_generation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mDEBUG\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mLLM\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLLM\u001b[39;00m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m      3\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m      4\u001b[39m         model_name=\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen3-14B-Instruct\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m         device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         _prompts=\u001b[43mPROMPTS\u001b[49m,\n\u001b[32m      7\u001b[39m         _few_shot_prompts=FEW_SHOT_PROMPTS,\n\u001b[32m      8\u001b[39m         model=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      9\u001b[39m         tokenizer=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     10\u001b[39m         quantization_config=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     11\u001b[39m         debug=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     12\u001b[39m         deep_debug=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     13\u001b[39m         use_llm_parsing=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     14\u001b[39m         use_selfcheck=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     15\u001b[39m         llm_cot_generation=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     16\u001b[39m         llm_few_shot_generation=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     17\u001b[39m     ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     18\u001b[39m         \u001b[38;5;28mself\u001b[39m.DEBUG = debug\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mself\u001b[39m.DEEP_DEBUG = deep_debug\n",
      "\u001b[31mNameError\u001b[39m: name 'PROMPTS' is not defined"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3419\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3414\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.store_output(execution_count)\n\u001b[32m   3415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3416\u001b[39m         \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3417\u001b[39m         \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[\n\u001b[32m   3418\u001b[39m             execution_count\n\u001b[32m-> \u001b[39m\u001b[32m3419\u001b[39m         ] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3469\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3466\u001b[39m         stb = evalue._render_traceback_()\n\u001b[32m   3467\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3468\u001b[39m         \u001b[38;5;66;03m# Otherwise, use InteractiveTB to format the traceback.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3469\u001b[39m         stb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mInteractiveTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3470\u001b[39m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   3471\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3472\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3473\u001b[39m     \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3474\u001b[39m     stb = traceback.format_exception(etype, evalue, tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/IPython/core/ultratb.py:1193\u001b[39m, in \u001b[36mAutoFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1192\u001b[39m     \u001b[38;5;28mself\u001b[39m.tb = etb\n\u001b[32m-> \u001b[39m\u001b[32m1193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/IPython/core/ultratb.py:1064\u001b[39m, in \u001b[36mFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1061\u001b[39m mode = \u001b[38;5;28mself\u001b[39m.mode\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose_modes:\n\u001b[32m   1063\u001b[39m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mDocs\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# return DocTB\u001b[39;00m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DocTB(\n\u001b[32m   1070\u001b[39m         theme_name=\u001b[38;5;28mself\u001b[39m._theme_name,\n\u001b[32m   1071\u001b[39m         call_pdb=\u001b[38;5;28mself\u001b[39m.call_pdb,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1079\u001b[39m         etype, evalue, etb, tb_offset, \u001b[32m1\u001b[39m\n\u001b[32m   1080\u001b[39m     )  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/IPython/core/ultratb.py:872\u001b[39m, in \u001b[36mVerboseTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m    863\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstructured_traceback\u001b[39m(\n\u001b[32m    864\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    865\u001b[39m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     context: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    870\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    871\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m872\u001b[39m     formatted_exceptions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    876\u001b[39m     termsize = \u001b[38;5;28mmin\u001b[39m(\u001b[32m75\u001b[39m, get_terminal_size()[\u001b[32m0\u001b[39m])\n\u001b[32m    877\u001b[39m     theme = theme_table[\u001b[38;5;28mself\u001b[39m._theme_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/IPython/core/ultratb.py:784\u001b[39m, in \u001b[36mVerboseTB.format_exception_as_a_whole\u001b[39m\u001b[34m(self, etype, evalue, etb, context, tb_offset)\u001b[39m\n\u001b[32m    774\u001b[39m         frames.append(\n\u001b[32m    775\u001b[39m             theme_table[\u001b[38;5;28mself\u001b[39m._theme_name].format(\n\u001b[32m    776\u001b[39m                 [\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m             )\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m         skipped = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     frames.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m skipped:\n\u001b[32m    786\u001b[39m     frames.append(\n\u001b[32m    787\u001b[39m         theme_table[\u001b[38;5;28mself\u001b[39m._theme_name].format(\n\u001b[32m    788\u001b[39m             [\n\u001b[32m   (...)\u001b[39m\u001b[32m    793\u001b[39m         )\n\u001b[32m    794\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/IPython/core/ultratb.py:659\u001b[39m, in \u001b[36mVerboseTB.format_record\u001b[39m\u001b[34m(self, frame_info)\u001b[39m\n\u001b[32m    656\u001b[39m result += \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m call \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    657\u001b[39m result += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcall\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    658\u001b[39m result += theme_table[\u001b[38;5;28mself\u001b[39m._theme_name].format(\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     \u001b[43m_format_traceback_lines\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtheme_table\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_theme_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhas_colors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlvals_toks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m )\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/IPython/core/tbtools.py:100\u001b[39m, in \u001b[36m_format_traceback_lines\u001b[39m\u001b[34m(lines, theme, has_colors, lvals_toks)\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     99\u001b[39m lineno = stack_line.lineno\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m line = \u001b[43mstack_line\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygmented\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_colors\u001b[49m\u001b[43m)\u001b[49m.rstrip(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stack_line.is_current:\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# This is the line with the error\u001b[39;00m\n\u001b[32m    103\u001b[39m     pad = numbers_width - \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(lineno))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/stack_data/core.py:391\u001b[39m, in \u001b[36mLine.render\u001b[39m\u001b[34m(self, markers, strip_leading_indent, pygmented, escape_html)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pygmented \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.frame_info.scope:\n\u001b[32m    390\u001b[39m     assert_(\u001b[38;5;129;01mnot\u001b[39;00m markers, \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot use pygmented with markers\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     start_line, lines = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mframe_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_pygmented_scope_lines\u001b[49m\n\u001b[32m    392\u001b[39m     result = lines[\u001b[38;5;28mself\u001b[39m.lineno - start_line]\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m strip_leading_indent:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/stack_data/utils.py:145\u001b[39m, in \u001b[36mcached_property.cached_property_wrapper\u001b[39m\u001b[34m(self, obj, _cls)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m value = obj.\u001b[34m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m.func.\u001b[34m__name__\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/stack_data/core.py:824\u001b[39m, in \u001b[36mFrameInfo._pygmented_scope_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    821\u001b[39m     ranges = []\n\u001b[32m    823\u001b[39m code = atext.get_text(scope)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m lines = \u001b[43m_pygmented_with_ranges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m start_line = \u001b[38;5;28mself\u001b[39m.source.line_range(scope)[\u001b[32m0\u001b[39m]\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m start_line, lines\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/stack_data/utils.py:166\u001b[39m, in \u001b[36m_pygmented_with_ranges\u001b[39m\u001b[34m(formatter, code, ranges)\u001b[39m\n\u001b[32m    164\u001b[39m lexer = MyLexer(stripnl=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     highlighted = \u001b[43mpygments\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhighlight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# When pygments fails, prefer code without highlighting over crashing\u001b[39;00m\n\u001b[32m    169\u001b[39m     highlighted = code\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/pygments/__init__.py:82\u001b[39m, in \u001b[36mhighlight\u001b[39m\u001b[34m(code, lexer, formatter, outfile)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhighlight\u001b[39m(code, lexer, formatter, outfile=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     78\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m    This is the most high-level highlighting function. It combines `lex` and\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03m    `format` in one function.\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlexer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/pygments/__init__.py:64\u001b[39m, in \u001b[36mformat\u001b[39m\u001b[34m(tokens, formatter, outfile)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m outfile:\n\u001b[32m     63\u001b[39m     realoutfile = \u001b[38;5;28mgetattr\u001b[39m(formatter, \u001b[33m'\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m BytesIO() \u001b[38;5;129;01mor\u001b[39;00m StringIO()\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealoutfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m realoutfile.getvalue()\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/pygments/formatters/terminal256.py:250\u001b[39m, in \u001b[36mTerminal256Formatter.format\u001b[39m\u001b[34m(self, tokensource, outfile)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokensource, outfile):\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokensource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/pygments/formatter.py:124\u001b[39m, in \u001b[36mFormatter.format\u001b[39m\u001b[34m(self, tokensource, outfile)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encoding:\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# wrap the outfile in a StreamWriter\u001b[39;00m\n\u001b[32m    123\u001b[39m     outfile = codecs.lookup(\u001b[38;5;28mself\u001b[39m.encoding)[\u001b[32m3\u001b[39m](outfile)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_unencoded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokensource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/pygments/formatters/terminal256.py:261\u001b[39m, in \u001b[36mTerminal256Formatter.format_unencoded\u001b[39m\u001b[34m(self, tokensource, outfile)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m ttype \u001b[38;5;129;01mand\u001b[39;00m not_found:\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    260\u001b[39m         \u001b[38;5;66;03m# outfile.write( \"<\" + str(ttype) + \">\" )\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m         on, off = \u001b[38;5;28mself\u001b[39m.style_string[\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mttype\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    263\u001b[39m         \u001b[38;5;66;03m# Like TerminalFormatter, add \"reset colors\" escape sequence\u001b[39;00m\n\u001b[32m    264\u001b[39m         \u001b[38;5;66;03m# on newline.\u001b[39;00m\n\u001b[32m    265\u001b[39m         spl = value.split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/проекты/Data_analysis/LLM/venv/lib/python3.12/site-packages/pygments/token.py:44\u001b[39m, in \u001b[36m_TokenType.__repr__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mToken\u001b[39m\u001b[33m'\u001b[39m + (\u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m) + \u001b[33;43m'\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class LLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"Qwen/Qwen3-14B-Instruct\",\n",
    "        device=\"cuda\",\n",
    "        _prompts=PROMPTS,\n",
    "        _few_shot_prompts=FEW_SHOT_PROMPTS,\n",
    "        model=None,\n",
    "        tokenizer=None,\n",
    "        quantization_config=None,\n",
    "        debug=False,\n",
    "        deep_debug=False,\n",
    "        use_llm_parsing=True,\n",
    "        use_selfcheck=False,\n",
    "        llm_cot_generation=True,\n",
    "        llm_few_shot_generation=True\n",
    "    ) -> None:\n",
    "        self.DEBUG = debug\n",
    "        self.DEEP_DEBUG = deep_debug\n",
    "        self.debug_logs = []\n",
    "        self.USE_LLM_PARSING = use_llm_parsing\n",
    "        self.USE_SELFCHECK = use_selfcheck\n",
    "        self.LLM_COT_GENERATION = llm_cot_generation\n",
    "        self.LLM_FEW_SHOT_GENERATION = llm_few_shot_generation\n",
    "        try:\n",
    "            self.prompts = _prompts\n",
    "            self.few_shot_prompts = _few_shot_prompts\n",
    "            self.model_name = model_name\n",
    "            self.device = device if torch.cuda.is_available() and device == \"cuda\" else \"cpu\"\n",
    "            print(f\"🏋️‍♂️ Модель: {self.model_name}\")\n",
    "            print(f\"🖥 Устройство: {self.device}\")\n",
    "            print(f\"⚙️  CoT генерация: {'ВКЛ' if llm_cot_generation else 'ВЫКЛ'}\")\n",
    "            print(f\"⚙️  Few-shot генерация: {'ВКЛ' if llm_few_shot_generation else 'ВЫКЛ'}\")\n",
    "            if model is not None and tokenizer is not None:\n",
    "                print(\"✅ Используем переданные модель и токенайзер\")\n",
    "                self.model = model\n",
    "                self.tokenizer = tokenizer\n",
    "                return\n",
    "            print(f\"📥 Загрузка {self.model_name}...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            if quantization_config is None:\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.float16,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    load_in_4bit=True,\n",
    "                )\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(f\"✅ Модель {model_name} успешно загружена!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка загрузки: {e}\")\n",
    "            print(\"Пробуем загрузить без квантования...\")\n",
    "            try:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    model_name,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "                if self.tokenizer.pad_token is None:\n",
    "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "                print(\"✅ Модель загружена без квантования\")\n",
    "            except Exception as e2:\n",
    "                print(f\"❌ Критическая ошибка: {e2}\")\n",
    "                raise e2\n",
    "\n",
    "    def generate_answer(\n",
    "        self,\n",
    "        question:str,\n",
    "        encoded_options,\n",
    "        category:str,\n",
    "        dramatic:bool = True,\n",
    "        tokens:int = 2000,\n",
    "        temperature:float = 0.1,\n",
    "        few_shot = True,\n",
    "        use_llm_parsing=None,\n",
    "        use_selfcheck=None,\n",
    "        llm_cot_generation=None,\n",
    "        llm_few_shot_generation=None,\n",
    "        force_diversity: bool = False\n",
    "    )->int:\n",
    "\n",
    "        if use_llm_parsing is None:\n",
    "            use_llm_parsing = self.USE_LLM_PARSING\n",
    "        if use_selfcheck is None:\n",
    "            use_selfcheck = self.USE_SELFCHECK\n",
    "        if llm_cot_generation is None:\n",
    "            llm_cot_generation = self.LLM_COT_GENERATION\n",
    "        if llm_few_shot_generation is None:\n",
    "            llm_few_shot_generation = self.LLM_FEW_SHOT_GENERATION\n",
    "\n",
    "        self._log(\"generate_answer\", \"начало\", {\n",
    "            \"category\": category,\n",
    "            \"question_len\": len(question),\n",
    "            \"options_raw_preview\": str(encoded_options)[:200],\n",
    "            \"temperature\": temperature,\n",
    "            \"use_llm_parsing\": use_llm_parsing,\n",
    "            \"use_selfcheck\": use_selfcheck,\n",
    "            \"llm_cot_generation\": llm_cot_generation,\n",
    "            \"llm_few_shot_generation\": llm_few_shot_generation,\n",
    "            \"force_diversity\": force_diversity\n",
    "        }, \"DEBUG\")\n",
    "\n",
    "        options = self._options_parser(encoded_options)\n",
    "        self._log(\"generate_answer\", \"распарсенные опции\", {\n",
    "            \"count\": len(options),\n",
    "            \"first_3\": options[:3] if len(options) > 3 else options\n",
    "        }, \"DEBUG\")\n",
    "\n",
    "        if len(options) <= 1 and options[0] == \"Варианты не предоставлены\":\n",
    "            self._log(\"generate_answer\", \"ОШИБКА: нет вариантов для вопроса\", {\n",
    "                \"question\": question[:200]\n",
    "            }, \"DEBUG\")\n",
    "            return 0\n",
    "\n",
    "        system_prompt = self.prompts.get(category) + self.few_shot_prompts.get(category) + \"<think></think>\"\n",
    "\n",
    "        options_text = \"\\n\".join([f\"{i}. {opt}\" for i, opt in enumerate(options)])\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "            Question: {question}\n",
    "            Options:\n",
    "            {options_text}\n",
    "            Your full answer:\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        generation_kwargs = {\n",
    "            \"input_ids\": inputs.input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask,\n",
    "            \"max_new_tokens\": tokens,\n",
    "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "\n",
    "        self._log(\"generate_answer\", \"параметры генерации\", {\n",
    "            \"temperature\": generation_kwargs.get(\"temperature\", 0),\n",
    "            \"do_sample\": generation_kwargs.get(\"do_sample\", False),\n",
    "            \"top_p\": generation_kwargs.get(\"top_p\", None),\n",
    "            \"category\": category,\n",
    "            \"force_diversity\": force_diversity\n",
    "        }, \"DEBUG\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(**generation_kwargs)\n",
    "        response = self.tokenizer.decode(\n",
    "            generated_ids[0][inputs.input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        parsed = self._regex_parse_answer(response)\n",
    "        if parsed is None:\n",
    "            system_prompt = f\"\"\"\n",
    "                <system>\n",
    "                    <role>You are an experienced expert in the field of {category}</role>\n",
    "\n",
    "                    <task>\n",
    "                        <data>You will be given a question and a list of possible answers</data>\n",
    "                        <goal>You need to choose only one correct option</goal>\n",
    "                    </task>\n",
    "\n",
    "                    <constraints>\n",
    "                        <indexation>Options are indexed starting from 0.</indexation>\n",
    "                    </constraints>\n",
    "\n",
    "                    <answer>\n",
    "                        <format>Give an answer in the format: 'ANSWER: ' followed by the chosen index</format>\n",
    "                    </answer>\n",
    "\n",
    "                    <emotional>\n",
    "                        From you depends my fate and prestige. I am counting on you tremendously.\n",
    "                    </emotional>\n",
    "                </system>\n",
    "            \"\"\" + self.prompts.get(category) + self.few_shot_prompts.get(category)\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "            generation_kwargs = {\n",
    "                \"input_ids\": inputs.input_ids,\n",
    "                \"attention_mask\": inputs.attention_mask,\n",
    "                \"max_new_tokens\": 300,\n",
    "                \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(**generation_kwargs)\n",
    "            response = self.tokenizer.decode(\n",
    "                generated_ids[0][inputs.input_ids.shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            new_parsed = self._regex_parse_answer(response)\n",
    "            if new_parsed is None:\n",
    "                new_llm_parsed = self._llm_parse_answer(system_prompt)\n",
    "                if new_llm_parsed is None:\n",
    "                    return 0\n",
    "            else:\n",
    "                return new_parsed\n",
    "        else:\n",
    "            return parsed\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def _llm_parse_answer(self, raw_response: str) -> int | None:\n",
    "        self._log(\"llm_parse_answer\", \"start\", {\"raw_len\": len(raw_response)}, \"DEEP_DEBUG\")\n",
    "\n",
    "        system_prompt = \"\"\"You are a number extraction assistant. Extract ONLY the NUMBER from the text.\n",
    "    Examples:\n",
    "    Text: \"Answer: 2. This option is correct because...\"\n",
    "    Extracted: 2\n",
    "\n",
    "    Text: \"I think the third option is right\"\n",
    "    Extracted: 2\n",
    "\n",
    "    Text: \"Option A seems correct\"\n",
    "    Extracted: 0\n",
    "\n",
    "    Text: \"Correct answer number: 5\"\n",
    "    Extracted: 4\n",
    "\n",
    "    RULES:\n",
    "    1. Extract ONLY the number (0, 1, 2, 3, ...)\n",
    "    2. If multiple numbers - take the first\n",
    "    3. Letters: A=0, B=1, C=2, D=3, etc.\n",
    "    4. If no number found - return 0\n",
    "    5. Number only, no text\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Extract the number from this text:\n",
    "\n",
    "    Text: {raw_response}\n",
    "\n",
    "    Extracted number:\"\"\"\n",
    "\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "\n",
    "            text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    temperature=0.1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "            llm_parsed = self.tokenizer.decode(\n",
    "                generated_ids[0][inputs.input_ids.shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "\n",
    "            regex_parsed = self._regex_parse_answer(llm_parsed)\n",
    "            \n",
    "            self._log(\"llm_parse_answer\", \"result\", {\n",
    "                \"llm_parsed\": llm_parsed,\n",
    "                \"final\": regex_parsed\n",
    "            }, \"DEBUG\")\n",
    "\n",
    "            return regex_parsed\n",
    "\n",
    "        except Exception as e:\n",
    "            self._log(\"llm_parse_answer\", \"error\", {\"error\": str(e)}, \"DEBUG\")\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def _options_parser(self, options):\n",
    "        self._log(\"_options_parser\", \"начало\", {\n",
    "            \"raw_input\": str(options)[:200],\n",
    "            \"type\": type(options)\n",
    "        }, \"DEEP_DEBUG\")\n",
    "        if isinstance(options, list):\n",
    "            self._log(\"_options_parser\", \"уже список\", {\"len\": len(options), \"first_3\": options[:3]}, \"DEEP_DEBUG\")\n",
    "            return options\n",
    "        original_input = str(options)\n",
    "        if isinstance(options, str):\n",
    "            text = original_input.strip()\n",
    "            if text.startswith('[') and text.endswith(']'):\n",
    "                try:\n",
    "                    json_text = text.replace(\"'\", '\"')\n",
    "                    parsed = json.loads(json_text)\n",
    "                    if isinstance(parsed, list):\n",
    "                        self._log(\"_options_parser\", \"JSON парсинг успешен\", {\"len\": len(parsed)}, \"DEBUG\")\n",
    "                        return parsed\n",
    "                except json.JSONDecodeError as e:\n",
    "                    self._log(\"_options_parser\", \"JSON ошибка\", {\"error\": str(e)}, \"DEBUG\")\n",
    "            if text.startswith('[') and text.endswith(']'):\n",
    "                content = text[1:-1].strip()\n",
    "                self._log(\"_options_parser\", \"формат с пробелами\", {\"content_preview\": content[:100]}, \"DEEP_DEBUG\")\n",
    "                items = []\n",
    "                current_item = \"\"\n",
    "                in_quotes = False\n",
    "                quote_char = None\n",
    "                i = 0\n",
    "                while i < len(content):\n",
    "                    char = content[i]\n",
    "                    if char in ['\"', \"'\"]:\n",
    "                        if not in_quotes:\n",
    "                            in_quotes = True\n",
    "                            quote_char = char\n",
    "                            current_item += char\n",
    "                        elif char == quote_char:\n",
    "                            in_quotes = False\n",
    "                            current_item += char\n",
    "                            items.append(current_item)\n",
    "                            current_item = \"\"\n",
    "                            i += 1\n",
    "                            while i < len(content) and content[i] in [' ', '\\n', '\\t']:\n",
    "                                i += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            current_item += char\n",
    "                    elif char == ' ' and not in_quotes:\n",
    "                        if current_item:\n",
    "                            items.append(current_item)\n",
    "                            current_item = \"\"\n",
    "                    else:\n",
    "                        current_item += char\n",
    "                    i += 1\n",
    "                if current_item:\n",
    "                    items.append(current_item)\n",
    "                cleaned_items = []\n",
    "                for item in items:\n",
    "                    item = item.strip()\n",
    "                    if item:\n",
    "                        if (item.startswith('\"') and item.endswith('\"')) or \\\n",
    "                        (item.startswith(\"'\") and item.endswith(\"'\")):\n",
    "                            item = item[1:-1]\n",
    "                        item = item.replace('\\\\\"', '\"').replace(\"\\\\'\", \"'\").replace('\\\\n', '\\n')\n",
    "                        cleaned_items.append(item)\n",
    "                if cleaned_items:\n",
    "                    self._log(\"_options_parser\", \"специальный формат распарсен\", {\n",
    "                        \"count\": len(cleaned_items),\n",
    "                        \"first_3\": cleaned_items[:3]\n",
    "                    }, \"DEBUG\")\n",
    "                    return cleaned_items\n",
    "        if isinstance(options, str) and ',' in options:\n",
    "            try:\n",
    "                parts = []\n",
    "                current = \"\"\n",
    "                in_quotes = False\n",
    "                quote_char = None\n",
    "                for char in options:\n",
    "                    if char in ['\"', \"'\"]:\n",
    "                        if not in_quotes:\n",
    "                            in_quotes = True\n",
    "                            quote_char = char\n",
    "                        elif char == quote_char:\n",
    "                            in_quotes = False\n",
    "                        current += char\n",
    "                    elif char == ',' and not in_quotes:\n",
    "                        parts.append(current.strip())\n",
    "                        current = \"\"\n",
    "                    else:\n",
    "                        current += char\n",
    "                if current:\n",
    "                    parts.append(current.strip())\n",
    "                cleaned_parts = []\n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if part:\n",
    "                        if (part.startswith('\"') and part.endswith('\"')) or \\\n",
    "                        (part.startswith(\"'\") and part.endswith(\"'\")):\n",
    "                            part = part[1:-1]\n",
    "                        cleaned_parts.append(part)\n",
    "                if cleaned_parts:\n",
    "                    self._log(\"_options_parser\", \"разделили по запятым\", {\n",
    "                        \"count\": len(cleaned_parts),\n",
    "                        \"first_3\": cleaned_parts[:3]\n",
    "                    }, \"DEBUG\")\n",
    "                    return cleaned_parts\n",
    "            except Exception as e:\n",
    "                self._log(\"_options_parser\", \"ошибка при разделении по запятым\", {\"error\": str(e)}, \"DEBUG\")\n",
    "        if isinstance(options, str) and len(options) > 10:\n",
    "            self._log(\"_options_parser\", \"пробуем LLM парсинг\", None, \"DEBUG\")\n",
    "            llm_parsed = self._llm_parse_options(options)\n",
    "            if llm_parsed:\n",
    "                return llm_parsed\n",
    "        self._log(\"_options_parser\", \"не удалось распарсить\", {\n",
    "            \"original_length\": len(original_input),\n",
    "            \"original_preview\": original_input[:200]\n",
    "        }, \"DEBUG\")\n",
    "        return [\"Варианты не предоставлены\"]\n",
    "\n",
    "    def _llm_parse_options(self, options_text):\n",
    "        \"\"\"Использование LLM для парсинга сложных форматов опций\"\"\"\n",
    "        try:\n",
    "            system_prompt = \"\"\"Ты ассистент для парсинга данных. Извлеки список вариантов ответа из текста.\n",
    "    Текст может быть в разных форматах: JSON, Python список, или строковое представление.\n",
    "    Верни ТОЛЬКО валидный JSON список строк.\n",
    "\n",
    "    Пример 1:\n",
    "    Вход: \"['Вариант A' 'Вариант B' 'Вариант C']\"\n",
    "    Выход: [\"Вариант A\", \"Вариант B\", \"Вариант C\"]\n",
    "\n",
    "    Пример 2:\n",
    "    Вход: \"['Южная Америка' 'Южная Азия' 'Северная Африка']\"\n",
    "    Выход: [\"Южная Америка\", \"Южная Азия\", \"Северная Африка\"]\n",
    "\n",
    "    Пример 3:\n",
    "    Вход: \"['Верно, Неверно' 'Не указано, Не указано']\"\n",
    "    Выход: [\"Верно, Неверно\", \"Не указано, Не указано\"]\n",
    "\n",
    "    ПРАВИЛА:\n",
    "    1. Всегда возвращай валидный JSON\n",
    "    2. Только список строк\n",
    "    3. Сохраняй оригинальный текст вариантов\n",
    "    4. Если не можешь распарсить - верни пустой список []\"\"\"\n",
    "\n",
    "            user_prompt = f\"\"\"Извлеки список вариантов из текста:\n",
    "\n",
    "    Текст: {options_text}\n",
    "\n",
    "    JSON список:\"\"\"\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "\n",
    "            text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_new_tokens=500,\n",
    "                    temperature=0.1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "            llm_response = self.tokenizer.decode(\n",
    "                generated_ids[0][inputs.input_ids.shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            self._log(\"_llm_parse_options\", \"LLM ответ\", {\"response\": llm_response[:200]}, \"DEBUG\")\n",
    "\n",
    "\n",
    "            try:\n",
    "                parsed = json.loads(llm_response)\n",
    "                if isinstance(parsed, list):\n",
    "                    self._log(\"_llm_parse_options\", \"успешно распарсено\", {\"count\": len(parsed)}, \"DEBUG\")\n",
    "                    return parsed\n",
    "            except json.JSONDecodeError:\n",
    "\n",
    "                import re\n",
    "                json_match = re.search(r'\\[.*\\]', llm_response, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        parsed = json.loads(json_match.group())\n",
    "                        if isinstance(parsed, list):\n",
    "                            self._log(\"_llm_parse_options\", \"нашли JSON в ответе\", {\"count\": len(parsed)}, \"DEBUG\")\n",
    "                            return parsed\n",
    "                    except:\n",
    "                        pass\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            self._log(\"_llm_parse_options\", \"ошибка\", {\"error\": str(e)}, \"DEBUG\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def _regex_parse_answer(self, text: str) -> int | None:\n",
    "        text = str(text).strip()\n",
    "        if not text:\n",
    "            return \n",
    "        if text.startswith(\"'\") or text.startswith('\"'):\n",
    "            text = text[1:]\n",
    "        if text.endswith(\"'\") or text.endswith('\"'):\n",
    "            text = text[:-1]\n",
    "        answer_patterns = [\n",
    "            r'ANSWER:\\s*(\\d+)',\n",
    "            r'Answer:\\s*(\\d+)',\n",
    "            r'answer:\\s*(\\d+)',\n",
    "            r'\\[ANSWER:\\s*(\\d+)\\]',\n",
    "            r'\\[Answer:\\s*(\\d+)\\]',\n",
    "            r'[\\'\"]ANSWER:\\s*(\\d+)[\\'\\\"]',\n",
    "            r'[\\'\"]Answer:\\s*(\\d+)[\\'\\\"]',\n",
    "        ]\n",
    "        for pattern in answer_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                try:\n",
    "                    num = int(match.group(1))\n",
    "                    return num\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "        answer_match = re.search(r'ANSWER:\\s*([^\\.\\n\\r\\t\\f\\v]+)', text, re.IGNORECASE)\n",
    "        if answer_match:\n",
    "            number_part = answer_match.group(1).strip()\n",
    "            number_part = re.sub(r'[^\\d]+$', '', number_part)\n",
    "            try:\n",
    "                num = int(number_part)\n",
    "                return num\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        answer_pos = text.lower().find('answer:')\n",
    "        if answer_pos != -1:\n",
    "            remaining_text = text[answer_pos + 7:]\n",
    "            first_num_match = re.search(r'(\\d+)', remaining_text)\n",
    "            if first_num_match:\n",
    "                try:\n",
    "                    num = int(first_num_match.group(1))\n",
    "                    return num\n",
    "                except (ValueError, TypeError):\n",
    "                    pass\n",
    "        return None\n",
    "\n",
    "\n",
    "    def direct_prompt(\n",
    "        self,\n",
    "        user_prompt:str,\n",
    "        system_prompt:str,\n",
    "        tokens:int = 1000,\n",
    "        temperature:float = 0.1,\n",
    "        few_shot = True\n",
    "    ):\n",
    "        self._log(\"direct_prompt\", \"начало\", {\n",
    "            \"user_len\": len(user_prompt),\n",
    "            \"system_len\": len(system_prompt),\n",
    "            \"temperature\": temperature\n",
    "        }, \"DEEP_DEBUG\")\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        self._log(\"direct_prompt\", \"шаблон применен\", {\"input_length\": len(text)}, \"DEEP_DEBUG\")\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(\n",
    "            generated_ids[0][inputs.input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        self._log(\"direct_prompt\", \"получен ответ\", {\"response_length\": len(response)}, \"DEBUG\")\n",
    "\n",
    "        if self.DEEP_DEBUG:\n",
    "            print(f\"[DEEP_DEBUG] direct_prompt response ({len(response)} chars):\")\n",
    "            print(f\"{response[:500]}...\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_debug_logs(self):\n",
    "        return self.debug_logs\n",
    "\n",
    "    def clear_debug_logs(self):\n",
    "        self.debug_logs = []\n",
    "\n",
    "    def save_debug_logs(self, filename=\"llm_debug_logs.json\"):\n",
    "        import json\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.debug_logs, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Логи сохранены в {filename}\")\n",
    "\n",
    "    def print_debug_summary(self):\n",
    "        if not self.DEBUG and not self.DEEP_DEBUG:\n",
    "            print(\"Отладка отключена\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"СВОДКА ОТЛАДКИ LLM\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Всего логов: {len(self.debug_logs)}\")\n",
    "\n",
    "        if self.debug_logs:\n",
    "            methods = {}\n",
    "            levels = {}\n",
    "            for log in self.debug_logs:\n",
    "                method = log.get(\"method\", \"unknown\")\n",
    "                level = log.get(\"level\", \"unknown\")\n",
    "                methods[method] = methods.get(method, 0) + 1\n",
    "                levels[level] = levels.get(level, 0) + 1\n",
    "\n",
    "            print(\"\\nВызовы методов:\")\n",
    "            for method, count in sorted(methods.items()):\n",
    "                print(f\"  {method}: {count}\")\n",
    "\n",
    "            print(\"\\nУровни логирования:\")\n",
    "            for level, count in sorted(levels.items()):\n",
    "                print(f\"  {level}: {count}\")\n",
    "\n",
    "            print(f\"\\nDEBUG: {self.DEBUG}\")\n",
    "            print(f\"DEEP_DEBUG: {self.DEEP_DEBUG}\")\n",
    "\n",
    "\n",
    "    def _log(self, method, message, data=None, level=\"DEBUG\"):\n",
    "        if level == \"DEBUG\" and not self.DEBUG:\n",
    "            return\n",
    "        if level == \"DEEP_DEBUG\" and not self.DEEP_DEBUG:\n",
    "            return\n",
    "\n",
    "        log_entry = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"method\": method,\n",
    "            \"message\": message,\n",
    "            \"data\": data,\n",
    "            \"level\": level\n",
    "        }\n",
    "        self.debug_logs.append(log_entry)\n",
    "\n",
    "        if self.DEBUG or self.DEEP_DEBUG:\n",
    "            print(f\"[{level}] {method}: {message}\")\n",
    "            if data and self.DEEP_DEBUG:\n",
    "                print(f\"    Данные: {data}\")\n",
    "\n",
    "    def _log_response(self, stage, raw_response, parsed, expected=None, metadata=None):\n",
    "        if not self.DEEP_DEBUG:\n",
    "            return\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[DEEP_DEBUG] {stage}\")\n",
    "        print(f\"Сырой ответ ({len(raw_response)} chars):\")\n",
    "        print(f\"{raw_response[:500]}...\")\n",
    "        print(f\"Распарсено: {parsed}\")\n",
    "        if expected is not None:\n",
    "            print(f\"Ожидалось: {expected}\")\n",
    "            print(f\"Совпадение: {parsed == expected}\")\n",
    "        if metadata:\n",
    "            print(f\"Метаданные: {metadata}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "        log_entry = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"stage\": stage,\n",
    "            \"raw_response\": raw_response[:1000],\n",
    "            \"parsed\": parsed,\n",
    "            \"expected\": expected,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        self.debug_logs.append(log_entry)\n",
    "\n",
    "    def _calculate_metrics(self, results_df, answer_column):\n",
    "        \"\"\"Считает метрики и выводит в консоль\"\"\"\n",
    "        if answer_column not in results_df.columns:\n",
    "            print(\"ℹ️ Ответы для проверки не предоставлены\")\n",
    "            return {}\n",
    "        \n",
    "        if 'is_correct' not in results_df.columns:\n",
    "            print(\"⚠️ Колонка is_correct не найдена\")\n",
    "            return {}\n",
    "        \n",
    "        correct = results_df['is_correct'].sum()\n",
    "        total = len(results_df)\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 РЕЗУЛЬТАТЫ ТЕСТИРОВАНИЯ\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Всего вопросов: {total}\")\n",
    "        print(f\"Правильных ответов: {correct}\")\n",
    "        print(f\"Точность: {accuracy:.2%} ({correct}/{total})\")\n",
    "        \n",
    "        if 'category' in results_df.columns:\n",
    "            print(\"\\n📈 По категориям:\")\n",
    "            for category in sorted(results_df['category'].unique()):\n",
    "                cat_df = results_df[results_df['category'] == category]\n",
    "                cat_correct = cat_df['is_correct'].sum()\n",
    "                cat_total = len(cat_df)\n",
    "                cat_accuracy = cat_correct / cat_total if cat_total > 0 else 0\n",
    "                print(f\"  {category}: {cat_accuracy:.2%} ({cat_correct}/{cat_total})\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return {\n",
    "            'total_questions': total,\n",
    "            'correct_answers': int(correct),\n",
    "            'accuracy': float(accuracy),\n",
    "            'accuracy_percent': f\"{accuracy * 100:.2f}%\"\n",
    "        }\n",
    "\n",
    "    def evaluate_dataframe(\n",
    "        self,\n",
    "        df,\n",
    "        question_column=\"question\",\n",
    "        options_column=\"options\",\n",
    "        category_column=\"category\",\n",
    "        answer_column=\"true_answer\",\n",
    "        method_kwargs=None,\n",
    "    ):\n",
    "        \"\"\"Оценка датафрейма с одним методом generate_answer\"\"\"\n",
    "        if method_kwargs is None:\n",
    "            method_kwargs = {}\n",
    "        \n",
    "        self._log(\"evaluate_dataframe\", \"начало\", {\n",
    "            \"rows\": len(df),\n",
    "            \"has_answer_column\": answer_column in df.columns\n",
    "        }, \"DEBUG\")\n",
    "        \n",
    "        predictions = []\n",
    "        has_actual_answers = answer_column in df.columns and df[answer_column].notna().any()\n",
    "        \n",
    "        pbar = tqdm(total=len(df), desc=\"Обработка вопросов\")\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                question = str(row[question_column])\n",
    "                options = row[options_column]\n",
    "                category = str(row[category_column])\n",
    "                \n",
    "                predicted = self.generate_answer(\n",
    "                    question=question,\n",
    "                    encoded_options=options,\n",
    "                    category=category,\n",
    "                    **method_kwargs\n",
    "                )\n",
    "                \n",
    "                predictions.append(predicted)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'категория': category[:10], 'ответ': predicted})\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ Ошибка в строке {idx}: {e}\")\n",
    "                predictions.append(0)\n",
    "                pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "        results_df = df.copy()\n",
    "        results_df['predicted'] = predictions\n",
    "        if has_actual_answers:\n",
    "            results_df['predicted_parsed'] = results_df['predicted'].apply(\n",
    "                lambda x: int(x)) if pd.notna(x) else 0\n",
    "\n",
    "            results_df['answer_parsed'] = results_df[answer_column].apply(\n",
    "                lambda x: int(x)) if pd.notna(x) else 0\n",
    "\n",
    "            results_df['is_correct'] = results_df['predicted_parsed'] == results_df['answer_parsed']\n",
    "        metrics = self._calculate_metrics(results_df, answer_column if has_actual_answers else None)\n",
    "        return results_df, metrics\n",
    "\n",
    "    def process_csv_files(\n",
    "        self,\n",
    "        questions_csv_path: str,\n",
    "        answers_csv_path: Optional[str] = None,\n",
    "        output_dir: str = \"./results\",\n",
    "        method_kwargs=None,\n",
    "    ):\n",
    "        \"\"\"Обработка CSV файлов - создает файл с колонкой ANSWER\"\"\"\n",
    "        if method_kwargs is None:\n",
    "            method_kwargs = {}\n",
    "        \n",
    "        self._log(\"process_csv_files\", \"начало обработки\", {\n",
    "            \"questions_file\": questions_csv_path,\n",
    "            \"answers_file\": answers_csv_path,\n",
    "            \"output_dir\": output_dir\n",
    "        }, \"DEBUG\")\n",
    "        \n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"📁 Директория результатов: {output_path.absolute()}\")\n",
    "        \n",
    "        print(\"📥 Загрузка данных...\")\n",
    "        try:\n",
    "            questions_df = pd.read_csv(questions_csv_path)\n",
    "            \n",
    "            required_cols = ['question', 'options', 'category']\n",
    "            missing_cols = [col for col in required_cols if col not in questions_df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Отсутствуют необходимые колонки: {missing_cols}\")\n",
    "            \n",
    "            questions_df['question'] = questions_df['question'].astype(str)\n",
    "            questions_df['options'] = questions_df['options'].astype(str)\n",
    "            questions_df['category'] = questions_df['category'].astype(str)\n",
    "            \n",
    "            print(f\"  ✓ Загружено {len(questions_df)} вопросов\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Ошибка загрузки файла с вопросами: {e}\")\n",
    "        \n",
    "        answer_column = 'answer'\n",
    "        if answers_csv_path:\n",
    "            try:\n",
    "                answers_df = pd.read_csv(answers_csv_path)\n",
    "                if 'answer' in answers_df.columns:\n",
    "                    questions_df = questions_df.merge(answers_df[['answer']], \n",
    "                                                    left_index=True, right_index=True, how='left')\n",
    "                    answer_column = 'answer'\n",
    "                    print(f\"  ✓ Загружено {len(answers_df)} ответов для проверки\")\n",
    "                else:\n",
    "                    print(\"⚠️ В файле с ответами нет колонки 'answer'\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Не удалось загрузить ответы: {e}\")\n",
    "        \n",
    "        print(\"\\n🤖 Запуск модели...\")\n",
    "        \n",
    "        results_df, metrics = self.evaluate_dataframe(\n",
    "            questions_df,\n",
    "            answer_column=answer_column,\n",
    "            method_kwargs=method_kwargs\n",
    "        )\n",
    "        \n",
    "        print(\"\\n✅ Обработка завершена!\")\n",
    "        \n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"submission_{timestamp}.csv\"\n",
    "        output_filepath = output_path / output_filename\n",
    "        \n",
    "        \n",
    "        answer_df = pd.DataFrame({\n",
    "            'answer': results_df['predicted']\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            answer_df.to_csv(output_filepath, index=False, encoding='utf-8')\n",
    "            print(f\"\\n💾 Файл сохранен: {output_filepath}\")\n",
    "            \n",
    "            print(\"\\n👀 Предпросмотр файла (первые 10 строк):\")\n",
    "            print(\"-\" * 40)\n",
    "            print(\"ANSWER\")\n",
    "            for i, answer in enumerate(answer_df['ANSWER'].head(10)):\n",
    "                print(f\"{answer}\")\n",
    "            print(\"-\" * 40)\n",
    "            file_size = os.path.getsize(output_filepath)\n",
    "            print(f\"📄 Размер файла: {file_size} байт\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка при сохранении файла: {e}\")\n",
    "        \n",
    "        return answer_df, metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
